\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{graphicx}
%\usepackage{latexsym}


%figures:
\usepackage{tikz}
\usetikzlibrary{trees,positioning,backgrounds}
%\usepackage{tikz-qtree}
\usepackage{subcaption}

%references and keeping floats in place
\usepackage[pdftex,pdfborder={0 0 0},unicode,breaklinks,hyperfootnotes=false,bookmarks]{hyperref}
\usepackage[section]{placeins}

\usepackage{amsmath} 
\renewcommand{\vec}[1]{\mathbf{#1}}

\title{Statistical Structure in Language Processing \\ Phrase based models}
\author{ Cristina G\^arbacea\\
  10407936 \\
  {\small \tt cr1st1na.garbacea@gmail.com} 
  \\\And
  Sara Veldhoen \\
10545298   \\
  {\small \tt sara.veldhoen@student.uva.nl} \\}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Currently in statistical machine translation the best performing systems rely on phrases or word pairs extracted from word aligned parallel corpora. Using a heuristic to extract the phrase pairs is done to select only the consistent pairs out of the many possible alternatives. For extracting bilingual phrase pairs from a word aligned training corpus we rely on the alignments provided by \textit{Giza++} and present and motivate our choice of phrase pair extraction algorithm. We explore the coverage of the phrase table, along with the conditional and joint probability estimates of the phrases and report on the results we obtained, along with possible improvements to our approach.
\end{abstract}

\section{Introduction}
In this paper we explore the utility of phrase based models inside a statistical machine translation system. As compared to our previous assignment where we only used word aligned models, in this assignment our goal is to build an efficient phrase pair extraction tool that would extract phrase pairs of up to length 4 from a given word-aligned parallel training corpus. 

Since word based models translate words as atomic units, they fall short of capturing dependencies between groups of words. 
Even more so in cases where each source word is aligned to exactly one target word.
 Phrase based models can overcome this limitation by treating phrases, sequences of words, as atomic translation units, in order to make use of local context in the translation process. Phrase based translation models give improved translations over the IBM models and are capable of giving state-of-the-art translations for many pairs of languages.


We implemented a phrase extraction algorithm that we compare to Moses, an existing statistical machine translation system that allows for automatically training translation models for any language pair. We introduce Moses and the evaluation metric we use, Bleu, in section \ref{existing}. In section \ref{phraseExtraction} follows a presentation of the phrase extraction algorithm we implemented and we offer an insight into how to get joint and conditional probability estimates.
In section \ref{Eval} we present the results we obtained and finally we conclude in section \ref{Concl}.

\section{Related Work}
%\subsection{Moses and Bleu}
\label{existing}
As outlined in \cite{moses}, phrase based statistical machine translation has emerged nowadays as the dominant paradigm in machine translation research. \textit{Moses} \cite{mosesurl} is an open source toolkit which contains tools for data processing and training language and translation models, as well as components for tunning these models and evaluation of the translated output. In order to avoid duplicates, \textit{Moses} makes use of \textit{Giza++} \cite{giza++} for word alignments and \textit{SRILM} \cite{srilm} for language modeling. 

The training pipeline and the decoder are the main components of the system. Parallel data is fed as input to the training process (typically preprocessed beforehand by ignoring long sentences and removing misaligned sentence pairs) and the translation correspondences between the two languages are inferred by determining word co-occurences. The training procedure is split into 9 distinct steps: after preparing the data, word alignments are taken from the intersection of bidirectional rules of \textit{Giza++} and some alignment points from the union of the two runs, based on a number of heuristics. In this manner it is possible to estimate the maximum likelihood translation table useful for phrase extraction. The extracted phrases are then scored by collecting counts and computing the phrase translation probabilities $\phi(e|f)$ for each foreign phrase $f$. A reordering model based on distance and a generation model built from the target side of the parallel corpus are both included in the final configuration file which is generated last and needed for decoding.

Given a trained translation model and a source sentence, the decoder will translate the source sentence into the target language by finding the highest scoring sentence in the target language which matches the source sentence. The resulting translations are evaluated by using the \textit{BLEU} metric to assess how close is the machine translation to the professional human translation. Scores are calculated for each sentence and then averaged over the whole corpus, which basically means that the more reference translations per sentence, the higher the score will be. On the other way round, instead of attempting to find the exact human judgment for every sentence, averaging individual sentence judgment errors over a test corpus confers reliability to this metric and make it correlate highly with human judgments.


\section{Phrase Extraction and weight estimation}
\label{phraseExtraction}

In this section, we present our approach to the extraction of phrase pairs from the corpus. We also compute coverage of the phrase table. The estimation of translation probabilities is done in both a conditional and joint fashion.

\paragraph{Phrase Extraction}
The number of possible phrase pairs per sentence pair is huge: each sentence can be partitioned in a vast amount of ways, and each partition could form a phrase pair with any partition in the paired sentence. 

In order to reduce the space, we consider only phrase pairs that are consistent with word alignments produced by IBM models.
As in \cite{Koehn:2010}, consistency is defined as follows:
\begin{align*}
\langle \bar{e},\bar{f}\rangle\text{ is consistent with }A &\Leftrightarrow\\
\forall e_i\in \bar{e}: &\langle e_i,f_j\rangle \in A \Rightarrow f_j \in \bar{f}, \\
\text{ and }\forall f_j\in \bar{f}: &\langle e_i,f_j\rangle \in A \Rightarrow e_i \in \bar{e}, \\
\text{ and }\exists e_i \in \bar{e}, f_j\in \bar{f}: &\langle e_i,f_j\rangle \in A.\\
\end{align*}

For this assignment, the symmetrized alignments of the corpus sentences were given. 
We base our extraction algorithm on the one presented in \cite[page 133]{Koehn:2010}.
We iterate over all windows up to a certain length in the English sentence, and find the foreign windows that are consistent given the alignment. For all valid pairs of windows, we extract the corresponding phrase pair. Note that we keep counts of the thus extracted phrase pairs, for efficient weight estimation in a later stage.

\paragraph{Coverage of the phrase table}
To investigate the usefulness of the extracted phrases for unknown text, we compute the coverage as compared to held-out data: a similar but independent parallel corpus. For this purpose, we run the same extraction algorithm on the held-out data. Then, we check for each extracted phrase pair whether it is in the phrase table. If it is not, however, we might be able to construct the phrase pair from phrases in the phrase table. This is a hard problem: each phrase in the held-out phrase pair of length $n$ can be split in $2^{n-1}$ ways. Moreover, any phrase part in the source can be aligned to any phrase part in the target, thus introducing a combinatoric problem. Fortunately, we restricted the extracted phrases to a length of 4, so that the number of parts that we need to combine is at most $4*4$, with $4!$ possible combinations.


 First, we create the possible splits for both the input and output phrase. Those are the input to the constructing algorithm.
Since the phrase table is stored as a mapping from target to source phrases, we start from the target side. 
We begin with one possible target split, and look up the leftmost phrase part in the table. If it translates to any part of the foreign phrase, we recursively call the building algorithm with the remainder of the target and source phrase. Whenever a target part cannot be translated, the search for this split option is abandoned. 

The coverage percentage is the relative number of phrase pairs in the heldout set that are either in the phrase table or can be constructed as described above.


\paragraph{Conditional Probability Estimates}
After having extracted the phrase pairs, we compute the conditional translation probability estimates for a foreign phrase $\overline{f}$ given an English phrase $\overline{e}$, using the following formula:

\begin{align*}
\phi(\overline{f}|\overline{e}) = \frac{count(\overline{e}, \overline{f})}{\sum_{\overline{f}_i} count(\overline{e},\overline{f}_i)}
\end{align*}

Here $count(\overline{e}, \overline{f})$ denotes in how many sentence pairs a specific phrase occurs and is extracted. To get relative frequency estimates, we normalize this value by the count of occurences of all phrase pairs containg the English phrase $\overline{e}$ inside the whole corpus.


\paragraph{Joint Probability Estimates}
In \cite{marcu2002} quite a different approach is taken to phrase based translation. The idea of a noisy channel, that a foreign sentence is a corrupted version of an original English sentence, is abandoned. Rather, the two sentences are considered different substantiation of a bag of concepts. In this framework, the probability of a phrase pair is a joint probability conditioned on a concept. In practice, we do not explicitly model the concept but view the phrase pair itself as a concept, so its weight is just the joint translation probability of the two phrases: $t(\bar{e},\bar{f})$.

The estimation of the translation probabilities in \cite{marcu2002} is done in an adapted version of expectation maximization. 
In our implementation, we do not consider all alignments of all possible phrases, but instead base the extraction of phrases on the symmetrized word alignments from IBM models. Therefore, we can gather the counts of these phrase pairs directly:

\begin{align*}
\phi(\overline{f},\overline{e}) &=\phi(\overline{f}|\overline{e}) \times \phi(\overline{e}) \\
&=\frac{count(\overline{e}, \overline{f})}{\sum_{\overline{f}_i, \overline{e}_i} count(\overline{e},\overline{f}_i)}
\end{align*}


\section{Experiments and Results}
\label{Eval}



\paragraph{Phrase table coverage}
The training data consisted of 100,000 parallel sentences, the held-out corpus had 2000 sentences. Out of 95,073 consistent phrase pairs in the held-out corpus, $31,5\%$ could be constructed using the phrase table. Intuitively, this is quite a bad performance. As many as 95\% of these were not directly extracted from the phrase table, but built from smaller parts.

We analyzed which phrase pairs would be built. We observed that the vast majority consisted of phrases with punctuation. Naturally, punctuation is often aligned in the symmetrized word alignments. The phrase extraction algorithm behaves greedily, in that it adds as much as possible to a consistent phrase pair, including punctuation. Although sequences of word can often contribute to meaningful phrase pairs, adding punctuation mainly appears to introduce a lot of data sparsity. 

%Do not forget to mention that there is a tendency to build prases which contain punctuation marks in the end and explain why this happens !!

\paragraph{Test set results}
Using the data provided to us for training consisting of 100K Dutch-English bilingual sentences, we used the \textit{Moses} implementation to train a traslation system. Word-alignments were obtained by running \textit{MGiza++}, since it supports multithreading which speeds up the alignment process. Phrase extraction and scoring, lexicalized reordering tables and the \textit{Moses} configuration file were created automatically by running the \verb|train-model.perl| script. After the training process completed, we tested the decoder on the given test data consisting of 2.000 Dutch sentences by translating each one of these sentences to English. Finally we evaluated our results by running the \textit{BLEU} script \verb|multi-bleu.perl| script. The results are presented in Table \ref{bleuresults}:


\begin{table}[h]
\centering
\begin{tabular}{ l l }
  \hline \hline
  \textbf{Bleu Score} & 24.68 \\
  \hline \hline
  \textbf{1-gram precision} & 59.5 \\
  \textbf{2-gram precision} & 31.4 \\
  \textbf{3-gram precision} & 19.0 \\
  \textbf{4-gram precision} & 11.8 \\
  \hline \hline
  \textbf{BP} & 0,970 \\
  \textbf{ratio} & 0,971 \\
  \textbf{hyp\_len} & 49.018 \\
  \textbf{ref\_len} & 50.488 \\
  \hline \hline
\end{tabular}
\caption{\textit{BLEU} Results on the test set, where the following abbreviations stand for: BP (brevity penalty) = min(1, number of output words / number of reference words, ratio = number of output words / number of reference words, hyp\_len = number of output words, ref\_len = number of reference words}
\label{bleuresults}
\end{table}

\section{Conclusion}
\label{Concl}

\begin{thebibliography}{}
%\bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
%Dan Gusfield.
%\newblock 1997.
%\newblock {\em Algorithms on Strings, Trees and Sequences}.
%\newblock Cambridge University Press, Cambridge, UK.

\bibitem[1]{Koehn:2010}
Philipp Koehn,
\newblock 2010.
\newblock {\em Statistical Machine Translation}.
\newblock Cambridge University Press.

\bibitem[2]{marcu2002}
{Daniel Marcu and William Wong},
\newblock 2002.
\newblock {\em A phrase-based, joint probability model for statistical machine translation}.
\newblock {Association for Computational Linguistics}.
%@inproceedings{marcu2002phrase,
%  title={A phrase-based, joint probability model for statistical machine translation},
%  author={Marcu, Daniel and Wong, William},
%  booktitle={Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10},
%  pages={133--139},
%  year={2002},
%  organization={Association for Computational Linguistics}
%}

\bibitem[3]{och1999}
{Franz Josef Och, Christoph Tillmann, and Hermann Ney},
\newblock 1999.
\newblock {\em Improved alignment models for statistical machine translation}
\newblock {Proceedings of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora}.

\bibitem[4]{mosesurl}
{Koehn, Philipp},
\newblock 2014.
\newblock {\em Statistical Machine Translation System. User Manual and Code Guide}
\newblock {\url{http://statmt.org/moses/manual/manual.pdf}}.

\bibitem[5]{moses}
{Koehn, Philipp, et al}
\newblock{2007}
\newblock{\em Moses: Open source toolkit for statistical machine translation}.
\newblock{Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions}.

\bibitem[6]{giza++}
{Franz Josef Och and Hermann Ney}
\newblock{2003}
\newblock{\em A Systematic Comparison of Various Statistical Alignment Models}.
\newblock{Computational Linguistics, 1:29, 19-51}.

\bibitem[7]{srilm}
{Andreas Stolcke}
\newblock{2002}
\newblock{\em SRILM - An Extensible Language Modeling Toolkit}.
\newblock{901--904}.

\bibitem[8]{bleu}
{Kishore Papineni and Salim Roukos and Todd Ward and Wei-Jing Zhu}
\newblock{2002}
\newblock{\em BLEU - A Method for Automatic Evaluation of Machine Translation}.
\newblock{Proceedings of the 40th Annual Meeting of the Association for ACL}.
\newblock{311--318}.


\end{thebibliography}

\end{document}
