\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
%\usepackage{latexsym}


%figures:
\usepackage{tikz}
\usetikzlibrary{trees,positioning,backgrounds}
%\usepackage{tikz-qtree}
\usepackage{subcaption}

%references and keeping floats in place
\usepackage[pdftex,pdfborder={0 0 0},unicode,breaklinks,hyperfootnotes=false,bookmarks]{hyperref}
\usepackage[section]{placeins}

\usepackage{amsmath} 
\renewcommand{\vec}[1]{\mathbf{#1}}

\title{Statistical Structure in Language Processing \\ IBM Model 1}
\author{ Cristina G\^arbacea\\
  10407936 \\
  {\small \tt cr1st1na.garbacea@gmail.com} 
  \\\And
  Sara Veldhoen \\
10545298   \\
  {\small \tt sara.veldhoen@student.uva.nl} \\}

\date{}

\begin{document}

\maketitle

\begin{abstract}
In this paper we present the theoretical notions underlying the IBM Model 1 we implemented and the Expectation Maximization algorithm we applied to train the given parallel Dutch - English corpus. 
%We show that this model can easily overfit and that maximizing the likelihood of the training data can result in low performance accuracy.% Do we?
We also came up with our own extension to the model, which we compare and evaluate against the basic IBM model 1. 
\end{abstract}

\section{Introduction}
Statistical alignment models are widely used nowadays in a variety of natural language processing applications, ranging from machine translation to question answering and information retrieval. The goal of finding the best English translation $\vec{e}$ of a foreign sentence $\vec{f}$ is modeled under the assumption of the \textit{noisy channel} hypothesis, which considers the foreign sentence as a ``corrupted" instance of the original English sentence. The English sentence which is at the source of its foreign counterpart is an unknown issue, and thus the task of translating $\vec{f}$ into $\vec{e}$ becomes one of maximizing the probability of the English sentence given the foreign sentence, $P(\vec{e}|\vec{f})$. According to Bayes' theorem, 

\begin{equation}
P(\vec{e}|\vec{f})= \frac{P(\vec{e})\cdot P(\vec{f}|\vec{e})}{P(\vec{f})}
\end{equation}

the translation problem can be expressed as:
\begin{equation}
P(\vec{e}|\vec{f})= arg \ \underset{\vec{e}}{max} \ P(\vec{e})\cdot P(\vec{f}|\vec{e})
\end{equation}

where $P(\vec{e})$ denotes the language model, that takes care of the fluency of the output, 
 and $P(\vec{e}|\vec{f})$ denotes the translation model, that makes sure the translation is adequate. 

In order to be able to approximate this probability, we need to make assumptions. In %cite IBM paper
a series of increasingly complex models is presented. We implemented the first model, hereafter IBM model 1. 
In what follows we present the this model in section \ref{IBM1}, together with the Expectation Maximization formula used for training this model. We came up with an improvement over this model, which we introduce in section \ref{Improvement}. Then comes an evaluation of the alignment quality for both models in section \ref{Eval} and finally we conclude in Section \ref{Concl}.




\section{IBM Model 1}
\label{IBM1}

The assumption underlying the IBM models is that translating text comes down to aligning the words 
%IBM models focus on the translation task only which assumes that word alignments exist between the words 
of the foreign and the English sentence, and translating the words independently.
The words of the English sentence are called \textit{cepts} and generally within the IBM models \emph{groups} of cepts cannot be aligned to \emph{groups} of words in the foreign sentence.  Rather, each cept  is aligned to one foreign word or, in case there is no foreign correspondent, to the so called \textit{null} word.
Because of this simplification, an alignment can be represented as a vector that has the same length as the English sentence.

Hence the translation probability $P(\vec{f}|\vec{e})$ can be rewritten as the sum over all possible alignments $\vec{a}$ of conditional probabilities $P(\vec{f}, \vec{a}|\vec{e})$:
\begin{align}
P(\vec{f}|\vec{e}) =& \sum_\vec{a} P(\vec{f},\vec{a}|\vec{e})\\
=&\sum_\vec{a}  P(\vec{f},\vec{a},m|\vec{e}) \\
=&\sum_\vec{a}  P(m|\vec{e})\times P(\vec{f}|\vec{a},m,\vec{e})\\
=&  P(m|\vec{e}) \sum_\vec{a} P(\vec{f}|\vec{a},m,\vec{e})\\
P(\vec{f}|\vec{a},m,\vec{e})=&\prod_{j=1}^{m} P(a_j|a_1^{j-1}, f_1^{j-1}, m, \vec{e}) \notag \\
& \times  P(f_j|a_1^j,f_1^{j-1},m,\vec{e})
\end{align}


%IBM models build upon one another in increasing order of complexity. IBM Model 1 is the simplest probabilistic generative model based on lexical translation which assumes a word-to-word mapping between the target and the source sentence. It is widely used in working with parallel bilingual corpora, aligning syntactic fragments and estimating phrase translation probabilities.

The translation probability of a foreign sentence $\vec{f}$ of length $l$ into an English sentence $\vec{e}$ of length $m$ that is aligned as $\vec{a}$ is defined as:

\begin{equation}
p(\vec{e},\vec{a}|\vec{f}) = \frac{\epsilon}{(l + 1)^m} \prod_{j=1}^{m}t(e_j|f(a_j))
\end{equation}

%Did the languages change position here? We were estimating p(f,a|e)

\section{EM Training Formula}
\label{EM}
The parameters of IBM Model 1 for a given pair of languages are estimated using the EM algorithm. It takes as training data a corpus of paired sentences, each pair made up of a sentence in one language and its translation in the other language. Each target word is assumed to be generated by exactly one source word, including the \textit{null} word. The initial step of EM consists in setting uniform translation probabilities $t(e|f)$ over the target vocabulary. During the E-step the model is applied to the data and the alignment probabilities are computed; during the M-step counts over all possible alignments are collected and weighted by their probability. 

+ training formula..

\section{Improvement Over IBM Model 1}
\label{Improvement}

In IBM Model 1, all alignments are equally probable. The Viterbi alignment can thus be found by simply choosing, for each word, the cept that has the highest translation probability. 

An improvement can be made by introducing a probability function over alignments: $p(a|m,l)$. 
For simplification, we assume independence of each alignment decision, such that we can compute $p(a_j|j,m,l)$ for each word.

This is also done in IBM Model 2. However,  instead of training the alignment probabilities, as in IBM Model 2, we use a heuristic function. We base this function on the assumption that a word in the input should stay close, i.e. we want to favor $a_j=j$ over $a_j>j$ and $a_j<j$. Furthermore, we want to favor choosing the null word %ref

\section{Experiments and Results}
\label{Eval}

\section{Conclusion}
\label{Concl}

\begin{thebibliography}{}
\bibitem[1]{}
Philipp Koehn
\newblock 2010.
\newblock {\em Statistical Machine Translation}.
\newblock Cambridge University Press.

\end{thebibliography}

\end{document}
